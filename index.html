<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
        integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.1/css/all.min.css" rel="stylesheet">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>

    <style>
        .main {
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        .code {
            font-family: 'IBM Plex Mono', monospace;
        }
    </style>

    <title>Adaptive BatchNorm</title>
</head>

<body>

    <div class="container w-75 main">
        <div class="row">
            <div class="col-lg-2">
            </div>
            <div class="col-lg-8" id="main-content">
                <div class="row text-center my-5" id="#">
                    <h1>Improving robustness against common corruptions by covariate shift adaptation</h1>
                </div>

                <!-- Begin author list-->
                <div class="row text-center mb-4">
                    <div class="col-sm-4 mb-4">
                        <a href="https:/stes.io" target="_blank">Steffen Schneider*</a>
                        <a href="mailto:steffen@bethgelab.org"><i class="far fa-envelope"></i></a>
                        <a href="https://stes.io" target="_blank"><i class="fas fa-link"></i></a>
                        University of Tübingen & IMPRS-IS
                    </div>
                    <div class="col-sm-4 mb-4">
                        Evgenia Rusak*
                        evgenia.rusak@bethgelab.org
                        University of Tübingen & IMPRS-IS
                    </div>
                    <div class="col-sm-4">
                        Luisa Eck<br>
                        LMU Munich
                    </div>
                    <div class="col-sm-4">
                        Oliver Bringmann<br>
                        University of Tübingen
                    </div>
                    <div class="col-sm-4">
                        Wieland Brendel<br>
                        University of Tübingen
                    </div>
                    <div class="col-sm-4">
                        Matthias Bethge<br>
                        University of Tübingen & Amazon
                    </div>
                </div>
                <!-- End author list-->

                <div class="row text-center">
                    <div class="col-sm-6 mb-4">
                        <h3>
                            <i class="far fa-sticky-note"></i>
                            Paper
                        </h3>
                    </div>
                    <div class="col-sm-6 mb-4">
                        <h3>
                            <i class="fab fa-github"></i>
                            Code
                        </h3>
                    </div>
                </div>
                <div class="row text-center">
                    <p>
                        <b>tl;dr:</b>
                        <span class="text-muted">When evaluating robustness, go beyond the assumption of a single sample
                            from the target domain. We provide a simple baseline improving the corruption error up to
                            14% points over a wide range of models, when access to more than a single sample is
                            possible.
                        </span>
                    </p>
                </div>

                <div class="row">
                    <h3>Abstract</h3>
                </div>
                <div class="row">
                    <p>
                        ImageNet-trained deep neural networks commonly use batch normalization and assume that
                        activation
                        statistics of the training set generalize to the test set. This assumption is violated in
                        out-of-distribution evaluation settings, such as the ImageNet-C common corruptions benchmark.
                        Here we
                        show that the activation statistics of a few unlabeled samples from the test set can help to
                        substantially improve performance on ImageNet-C. Using the corrected statistics, ResNet-50
                        models reach
                        <a href="#">62.4% mCE on ImageNet-C compared to 76.7% without adaptation</a>. Using the more
                        robust
                        AugMix model, we improve the state of the art (without adaptation) <a href="#">from 56.5% mCE to
                            51.0%
                            mCE</a>. Even a single sample suffices to improve baseline results for the ResNet-50 and
                        AugMix
                        models, and 32 samples are sufficient to improve the current state of the art for a ResNet-50
                        architecture. We show that this improvement is consistent across 25 different popular computer
                        vision
                        models and argue that results with adapted statistics should be included whenever reporting
                        scores in
                        out-of-distribution generalization settings.
                    </p>
                </div>

                <div class="row">
                    <h3>Method</h3>
                </div>

                <div class="rounded w-100"
                    style="background-image: url(https://github.com/hendrycks/robustness/raw/master/assets/imagenet-c.png); background-repeat: no-repeat; background-size: contain, cover; min-height: 300px;">
                </div>


                <div class="row">
                    <p>
                        We investigate improvements by estimating statistics on the test dataset.
                    </p>

                    <p class="text-center">
                        $$
                        \bar{\mu} = \frac{N \mu_s}{N + n} + \frac{n \mu_t}{N + n} , \quad
                        \bar{\sigma}^2 = \frac{N \sigma_s^2}{N + n} + \frac{n \sigma_t^2}{N + n} .
                        $$
                    </p>
                </div>

                <div class="row">
                    <h3>Key Results</h3>
                </div>


                <div class="row">
                    <h4>Adaptation boosts robustness of a vanilla trained ResNet-50 model</h4>
                    <p>
                        We found that the robustness of computer vision models based on the ResNet-50 architecture is
                        currently underestimated:
                        When employing the corrected statistics to normalize the network, we see performance
                        improvements of a much as 14% points mCE.
                        <span data-footnote="1"></span>
                    </p>
                    <p>
                        The exact value depends on the target domain batchsize and the source domain pseudo-batchsize.
                        The following plot shows the dependency of performance on IN-C and number of unlabeled target
                        domain samples we can access.
                    </p>
                </div>
                <div class="row text-center align-center">
                    <div class="col-lg-10">
                        <canvas id="canvas-batchsize"></canvas>
                    </div>
                </div>


                <div class="row">
                    <h4>Adaptation consistently improves corruption robustness across IN trained models</h4>
                    <p>
                        We benchmarked the improvement of model robustness for all computer vision architectures
                        implemented in
                        the torchvision library.
                        We find consistent improvements, typically on the order of 10% points, when using the proposed
                        adaptation scheme.
                        <span data-footnote="2"></span>
                    </p>
                </div>
                <div class="row text-center align-center">
                    <canvas id="canvas-torchvision"></canvas>
                </div>

                <div class="row">
                    <h4>Adaptation yields new state of the art on IN-C for robust models.</h4>
                    <p>
                        Besides vanilla trained ResNets, we also explore a variety of robust models part of the <a
                            href="https://github.com/hendrycks/robustness#imagenet-c-leaderboard" target="_blank">IN-C
                            Leaderboard</a>.
                        We see considerable improvement across all of these models for a sufficient number of samples
                        from the target domain.
                        It is instructive to also consider the full trade-off between the number of samples in the
                        target domain and the performance.
                    </p>
                    <div class="row text-center align-center">
                        <div class="col-lg-12">
                            <canvas id="canvas-priors" class="mx-auto"></canvas>
                        </div>
                        <div class="btn-group align-center text-center mx-auto" role="group" aria-label="Basic example">
                            <button type="button" class="btn btn-secondary" id="select_sin">SIN</button>
                            <button type="button" class="btn btn-secondary" id="select_ant">ANT</button>
                            <button type="button" class="btn btn-secondary" id="select_antsin">ANT+SIN</button>
                            <button type="button" class="btn btn-secondary" id="select_augmix">AugMix</button>
                            <button type="button" class="btn btn-secondary" id="select_resnet">ResNet-50</button>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <h4>Large scale pre-training alleviates the need for adaptation.</h4>
                </div>

                <div class="row">
                    <h4>The Wasserstein Distance is a good measure of domain shift.</h4>
                    <p>How can we quantify domain shift besides estimating the classification performance on the target
                        domain?
                        Without target domain labels, we can leverage the mismatch in source and target statistics for
                        this purpose.
                        We define the normalized Wasserstein distance as
                        $$
                        W_2 = ...
                        $$
                        and consider the correlation between the Wasserstein distance and the error rates.
                        We find a correlation between the quantities especially <i>within the same corruption type</i>.
                    </p>
                </div>

                <div class="row">
                    <h3>Citation</h3>
                </div>
                <div class="row">
                    <p>If you find our analysis helpful please cite our pre-print:</p>
                </div>
                <div class="row justify-content-md-center">
                    <div class="col-sm-10 rounded p-3 m-2" style="background-color:lightgray;">
                        <p class="code">
                            @article{schneider2020betterinc,<br>
                            author = { Schneider, Steffen and Rusak, Evgenia<br>
                            and Eck, Luisa and Bringmann, Oliver<br>
                            and Brendel, Wieland and Bethge, Matthias},<br>
                            title = {Removing covariate shift improves<br>
                            robustness against common corruptions},<br>
                            journal = {CoRR},<br>
                            year = {2020},<br>
                            }
                        </p>
                    </div>
                </div>

                <div class="row">
                    <a href="#"><i class="fas fa-sort-up"></i></a>
                </div>

            </div>
            <div class="col-lg-2">
                <p data-footnote="1">FOOTNOTE 1</p>
                <p data-footnote="2">FOOTNOTE 2</p>
            </div>
        </div>

    </div>

    <footer>
        Webpage designed by Steffen Schneider using Bootstrap 4.5 and Chart.js.
    </footer>



    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

    <script src="data.js"></script>

    <script src="plot-bsz.js"></script>
    <script src="plot-torchvision.js"></script>
    <script src="plot-all-models.js"></script>

    <script>

        window.onload = function () {
            window.plotBatchsize = new Chart(
                document.getElementById('canvas-batchsize').getContext('2d'),
                config_batchsize);
            window.plotTorchvision = new Chart(
                document.getElementById('canvas-torchvision').getContext('2d'),
                config_torchvision);
            window.plotPriors = new Chart(
                document.getElementById('canvas-priors').getContext('2d'),
                config_priors);

            [sin, ant, antsin, augmix, resnet].forEach(func => {
                $(`#select_${func.name}`).click(function () {
                    new_dataset = get_datasets_priors(func);
                    window.plotPriors.config.data.datasets.forEach((ds, idx) => {
                        //ds.data.length = 0;
                        //new_dataset[idx].data.forEach(sample => ds.data.push(sample));
                        ds.data = new_dataset[idx].data;
                    });
                    window.plotPriors.update({ duration: 800, easing: 'easeInOutCubic' });
                });
            });
        };
    </script>

    <script>
    $(document).ready(function() {
        var currentCitationPosition = 0;

        // For each citation reference in the text.
        $('div.main-content span[data-footnote]').each(function(index) {
            console.log(index);
            // Add the citation number.
            var citationNum = $(this).data('footnote');
            $(this).html('(' + citationNum + ')');

            // Find the position of the citation reference, but don't overdraw an existing citation.
            var offset = $(this).offset();
            if (offset.top > currentCitationPosition) {
            currentCitationPosition = offset.top;
            }

            // Find the citation matching the citation reference.
            var citation = $('div.footnote p[data-footnote="' + citationNum + '"]');
            if (citation) {
            // Set this position to the offset.top of the reference, or below the previous citation.
            $(citation).offset({
                top: currentCitationPosition
            });
            // Pad by the height of the current citation to prevent overdraw.
            currentCitationPosition += $(citation).height();
            }

        });

    });
    </script>
</body>

</html>